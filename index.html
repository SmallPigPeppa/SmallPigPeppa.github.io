<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Wenzhuo Liu</title>
</head>
<body style="width: 1100px; margin: 0 auto;">
<div id="fwtitle">
<div id="toptitle">
<h1>Wenzhuo Liu</h1>
</div>
</div>
<div id="layout-content">
<table class="imgtable"><tr><td>
<img src="wenzhuoliu.jpg" alt="photo_me" width="130px" height="180px" />&nbsp;</td>
<td align="left"><p>Assistant Professor [<a href="https://scholar.google.com/citations?user=fjZ1CBwAAAAJ&hl=zh-CN">Google Scholar</a>] [<a href="https://github.com/Impression2805">Github</a>] <br />
Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science & Innovation <br /> 
Chinese Academy of Sciences</p>
<p>3/F, 17W, Science Park West Avenue, Hong Kong Science Park, Hong Kong.<br />  
Email: zhfei2018@gmail.com</a>

</p>
</td></tr></table>
<h2>About me</h2>
<p>I am currently an Assistant Professor at the Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science & Innovation, Chinese Academy of Sciences, working with Prof. <a href="https://zhaoxiangzhang.net/">Zhaoxiang Zhang</a>. I received my Ph.D. in Pattern Recognition and Intelligent Systems from
the Institute of Automation, Chinese Academy of Sciences, where I was advised by Prof. <a href="http://www.nlpr.ia.ac.cn/liucl/">Cheng-Lin Liu</a> and Prof. <a href="https://people.ucas.edu.cn/~xuyaozhang">Xu-Yao Zhang</a>. Prior to this, I received the B.E. degree from Tsinghua University.
</p>
<p>
Research Highlights: My research interests include topics in <b>reliability</b> (e.g., uncertainty estimation and calibration, failure detection) and <b>adaptability</b> (e.g., continual learning, novel class discovery) of mechine learning models (especially foundation models such as LVMs, LLMs and MLMsï¼‰in the open world/environment applications.  <br />
</p>
<p>
I am actively seeking highly motivated students. If you are interested, please send me an email with your CV. <br />
</p>
<!--
<h2>News & Updates </h2> 
<ul>

<div style="height:80px;width:fit-content;overflow:auto;background:#FFFFFF;">
	<li>
		<p>[2023/02/28] One paper on <b>misclassification detection</b> is accepted by <b>CVPR</b> 2023, code is coming soon.
			</p>
	</li>
</div>
</ul>
-->

</p>

<h2>Preprints </h2>
<ul>
<li><p><i>TrustLoRA: Low-Rank Adaptation for Failure Detection under Out-of-distribution Data</i>. ArXiv 2025 [<a href="">paper</a>].<br />
<b>Fei Zhu</b>, Xu-Yao Zhang, Zhaoxiang Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
	
<ul>
<li><p><i>Global Convergence of Continual Learning on Non-IID Data</i>. ArXiv 2025 [<a href="https://arxiv.org/pdf/2503.18511">paper</a>].<br />
<b>Fei Zhu</b>, Yujing Liu, Wenzhuo Liu, Zhaoxiang Zhang. <br /></p>
</li>
</ul>
	
<ul>
<li><p><i>PASS++: A Dual Bias Reduction Framework for Non-Exemplar Class-Incremental Learning</i>. ArXiv 2024 [<a href="https://arxiv.org/pdf/2407.14029">paper</a>].<br />
<b>Fei Zhu</b>, Xu-Yao Zhang, Zhen Cheng, Cheng-Lin Liu. <br /></p>
</li>
</ul>
	
<ul>
<li><p><i>Open-world machine learning: A review and new outlooks</i>. ArXiv 2024 [<a href="https://arxiv.org/pdf/2403.01759">paper</a>].<br />
<b>Fei Zhu</b>, Shijie Ma, Zhen Cheng, Xu-Yao Zhang, Zhaoxiang Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>

<ul>
<li><p><i>Towards Non-Exemplar Semi-Supervised Class-Incremental Learning</i>. ArXiv 2024 [<a href="https://arxiv.org/pdf/2403.18291">paper</a>].<br />
Wenzhuo Liu, <b>Fei Zhu</b>, Cheng-Lin Liu. <br /></p>
</li>
</ul>

<ul>
<li><p><i>Branch-Tuning: Balancing Stability and Plasticity for Continual Self-Supervised Learning</i>. ArXiv 2024 [<a href="https://arxiv.org/pdf/2403.18266">paper</a>].<br />
Wenzhuo Liu, <b>Fei Zhu</b>, Cheng-Lin Liu. <br /></p>
</li>
</ul>

<ul>
<li><p><i>Multi-scale Unified Network for Image Classification</i>. ArXiv 2024 [<a href="https://arxiv.org/pdf/2403.18294">paper</a>].<br />
Wenzhuo Liu, <b>Fei Zhu</b>, Cheng-Lin Liu. <br /></p>
</li>
</ul>

<ul>
<li><p><i>DESIRE: Dynamic Knowledge Consolidation for Rehearsal-Free Continual Learning</i>. ArXiv 2025 [<a href="https://arxiv.org/abs/2411.19154">paper</a>].<br />
Haiyang Guo, <b>Fei Zhu</b>, Fan-hu Zeng, Bing Liu, Xu-Yao Zhang. <br /></p>
</li>
</ul>


<ul>
<li><p><i>Dual-Modality Guided Prompt for Continual Learning of Large Multimodal Models</i>. ArXiv 2025 [<a href="https://arxiv.org/abs/2410.05849">paper</a>].<br />
Fan-hu Zeng, <b>Fei Zhu</b>, Haiyang Guo, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
	


<h2>Selected Publications </h2>
<ul>
<li><p>[TPAMI 2025] <i>ProtoGCD: Unified and Unbiased Prototype Learning for Generalized Category Discovery</i> [<a href="https://ieeexplore.ieee.org/abstract/document/10948388">paper</a>] [<a href="https://github.com/mashijie1028/ProtoGCD">code</a>].<br />
Shijie Ma, <b>Fei Zhu</b>, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p><i>[ICLR 2025] C-CLIP: Multimodal Continual Learning for Vision-Language Model</i> [<a href="https://openreview.net/forum?id=sb7qHFYwBc">paper</a>].<br />
Wen-Zhuo Liu, <b>Fei Zhu</b><sup>ðŸ“§</sup>, Longhui Wei, Qi Tian. <br /></p>
</li>
</ul>
<ul>
<li><p><i>[TNNLS 2025] Average of Pruning: Improving Performance and Stability of Out-of-Distribution Detection</i> [<a href="https://arxiv.org/abs/2303.01201">paper</a>].<br />
Zhen Cheng, <b>Fei Zhu</b>, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[NeurIPS 2024] <i>MSPE: Multi-Scale Patch Embedding Prompts Vision Transformers to Any Resolution</i> [<a href="https://arxiv.org/pdf/2405.18240">paper</a>].<br />
Wenzhuo Liu, <b>Fei Zhu</b>, Shijie Ma, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[NeurIPS 2024] <i>Happy: A Debiased Learning Framework for Continual Generalized Category Discovery</i> [<a href="https://proceedings.neurips.cc/paper/2021/file/77ee3bc58ce560b86c2b59363281e914-Paper.pdf">paper</a>] [<a href="https://github.com/mashijie1028/Happy-CGCD">code</a>]. <br />
Shijie Ma, <b>Fei Zhu</b>, Zhun Zhong, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[IJCV 2024] <i>Breaking the Limits of Reliable Prediction via Generated Data</i> [<a href="https://link.springer.com/article/10.1007/s11263-024-02221-5">paper</a>].<br />
Zhen Cheng, <b>Fei Zhu</b>, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[PR 2024] <i>Towards trustworthy dataset distillation</i> [<a href="https://www.sciencedirect.com/science/article/pii/S0031320324006265">paper</a>] [<a href="https://github.com/mashijie1028/TrustDD/">code</a>].<br />
Shijie Ma, <b>Fei Zhu</b>, Zhen Cheng, Xu-Yao Zhang. <br /></p>
</li>
</ul>
<ul>
<li><p>[ECCV 2024] <i>PILoRA: Prototype Guided Incremental LoRA for Federated Class-Incremental Learning</i> [<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08185.pdf">paper</a>] [<a href="https://arxiv.org/abs/2401.02094">arxiv</a>] [<a href="https://github.com/Ghy0501/PILoRA">code</a>].<br />
Haiyang Guo, <b>Fei Zhu</b>, Wenzhuo Liu, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[CVPR 2024] <i>RCL: Reliable Continual Learning for Unified Failure Detection</i> [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_RCL_Reliable_Continual_Learning_for_Unified_Failure_Detection_CVPR_2024_paper.pdf">paper</a>] [<a href="https://github.com/Impression2805/RCL">code</a>].<br />
<b>Fei Zhu</b>, Zhen Cheng, Xu-Yao Zhang, Cheng-Lin Liu, Zhaoxiang Zhang. <br /></p>
</li>
</ul>
<ul>
<li><p>[CVPR 2024] <i>Active Generalized Category Discovery</i> [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_Active_Generalized_Category_Discovery_CVPR_2024_paper.pdf">paper</a>] [<a href="https://arxiv.org/abs/2403.04272">arxiv</a>] [<a href="https://github.com/mashijie1028/ActiveGCD">code</a>].<br />
Shijie Ma, <b>Fei Zhu</b>, Zhun Zhong, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[TPAMI 2024] <i>Revisiting Confidence Estimation: Towards Reliable Failure Prediction</i> [<a href="https://ieeexplore.ieee.org/abstract/document/10356834">paper</a>] [<a href="https://arxiv.org/pdf/2403.02886.pdf">arxiv</a>] [<a href="https://github.com/Impression2805/FMFP">code</a>].<br />
<b>Fei Zhu</b>, Xu-Yao Zhang, Zhen Cheng, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[TPAMI 2023] <i>Learning by Seeing More Classes</i> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9964413">paper</a>].<br />
<b>Fei Zhu</b>, Xu-Yao Zhang, Rui-Qi Wang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[Neural Networks 2023] <i>Imitating the Oracle: Towards Calibrated Model for Class Incremental Learning</i> [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608023001879">paper</a>] [<a href="https://github.com/Impression2805/ItO4CIL">code</a>].<br />
<b>Fei Zhu</b>, Zhen Cheng, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[CVPR 2023 <font color="red">Highlight Paper</font> (Top 2.5%)] <i>OpenMix: Exploring Outlier Samples for Misclassification Detection</i> [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_OpenMix_Exploring_Outlier_Samples_for_Misclassification_Detection_CVPR_2023_paper.pdf">paper</a>] [<a href="https://arxiv.org/pdf/2303.17093.pdf">arxiv</a>] [<a href="https://github.com/Impression2805/OpenMix">code</a>].<br />
<b>Fei Zhu</b>, Zhen Cheng, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[ECCV 2022] <i>Rethinking Confidence Calibration for Failure Prediction</i> [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136850512.pdf">paper</a>] [<a href="https://arxiv.org/pdf/2303.02970v1.pdf">arxiv</a>] [<a href="https://github.com/Impression2805/FMFP">code</a>].<br />
<b>Fei Zhu</b>, Zhen Cheng, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[CVPR 2021 <font color="red">Oral Paper</font> (Top 4%)] <i>Prototype Augmentation and Self-Supervision for Incremental Learning</i> [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhu_Prototype_Augmentation_and_Self-Supervision_for_Incremental_Learning_CVPR_2021_paper.pdf">paper</a>] [<a href="https://github.com/Impression2805/CVPR21_PASS">code</a>].<br />
<b>Fei Zhu</b>, Xu-Yao Zhang, Chuang Wang, Fei Yin, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[NeurIPS 2021] <i>Class-Incremental Learning via Dual Augmentation</i> [<a href="https://proceedings.neurips.cc/paper/2021/file/77ee3bc58ce560b86c2b59363281e914-Paper.pdf">paper</a>] [<a href="https://github.com/Impression2805/IL2A">code</a>]. <br />
<b>Fei Zhu</b>, Zhen Cheng, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[IEEE/CAA JAS 2023 <font color="red">Invited Reviews</font>] <i>Class Incremental Learning: A Review and Performance Evaluation (In Chinese)</i> [<a href="http://www.aas.net.cn/cn/article/doi/10.16383/j.aas.c220588?viewType=HTML">paper</a>].<br />
<b>Fei Zhu</b>, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[PR 2023] <i>Adversarial Training with Distribution Normalization and Margin Balance</i> [<a href="https://www.sciencedirect.com/science/article/pii/S0031320322006616">paper</a>].<br />
Zhen Cheng, <b>Fei Zhu</b>, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[Nature Communications 2022 <font color="red">Highlight Paper</font>] <i>Decoding lip language using triboelectric sensors with deep learning</i> [<a href="https://www.nature.com/articles/s41467-022-29083-0">paper</a>].<br />
Yi-Jia Lu*, Han Tan*, Jia Cheng*, <b>Fei Zhu</b>, Bing Liu, Shan-Shan Wei, LinHong Ji, Zhong-Lin Wang. <br /></p>
</li>
</ul>

<h2>Academic Services</h2> 
<ul>
<li><p> Conference Reviewer: NeurIPS, ICLR, ICML, CVPR, ICCV, ECCV </p></li>
<li><p> Journal Reviewer: IEEE TIP, TNNLS, TMM, TKDE, PR, NN, IJCV </p></li>
<li><p> Workshop Organizer: Trustworthy Model and Learning in Open Environment, PRCV 2024 </p></li>
</ul>

<h2>Talk</h2> 
<ul>
<li><p> Unknown Rejection in Open Environment, Biomedical Engineering Distinguished Lecture Series, Southern University of Science and Technology, August, 2024 </p></li>
<li><p> Deep Continual Learning, School of Computer Science and Engineering, Nanjing University of Science and Technology, January, 2025  </p></li>
<li><p> Open-Environment Continual Learning, Forum of Zhongguancun College, Beijing, February, 2025  </p></li>
</ul>

</div>
</body>
</html>

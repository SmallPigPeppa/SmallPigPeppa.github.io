<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Wenzhuo Liu</title>
</head>
<body style="width: 1100px; margin: 0 auto;">
<div id="fwtitle">
<div id="toptitle">
<h1>Wenzhuo Liu</h1>
</div>
</div>
<div id="layout-content">
<table class="imgtable"><tr><td>
<img src="wenzhuoliu.jpg" alt="photo_me" width="105px" height="150px" />&nbsp;</td>
<td align="left"><p>Assistant Professor [<a href="https://scholar.google.com/citations?user=toVhUOgAAAAJ&hl=en">Google Scholar</a>] [<a href="https://github.com/SmallPigPeppa">Github</a>] <br />
Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science & Innovation <br /> 
Chinese Academy of Sciences</p>
<p>3/F, 17W, Science Park West Avenue, Hong Kong Science Park, Hong Kong.<br />  
Email: liuwenzhuo2020@ia.ac.cn</a>

</p>
</td></tr></table>
<h2>About me</h2>
<p>I am currently an Assistant Professor at the Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science & Innovation, Chinese Academy of Sciences, working with Prof. <a href="https://zhaoxiangzhang.net/">Zhaoxiang Zhang</a>. I received my Ph.D. in Pattern Recognition and Intelligent Systems from
the Institute of Automation, Chinese Academy of Sciences, where I was advised by Prof. <a href="http://www.nlpr.ia.ac.cn/liucl/">Cheng-Lin Liu</a> and Prof. <a href="https://people.ucas.edu.cn/~xuyaozhang">Xu-Yao Zhang</a>. Prior to this, I received the B.E. degree from Tsinghua University.
</p>
<p>
Research Highlights: My research interests include topics in <b>reliability</b> (e.g., uncertainty estimation and calibration, failure detection) and <b>adaptability</b> (e.g., continual learning, novel class discovery) of mechine learning models (especially foundation models such as LVMs, LLMs and MLMsï¼‰in the open world/environment applications.  <br />
</p>
<p>
I am actively seeking highly motivated students. If you are interested, please send me an email with your CV. <br />
</p>
<!--
<h2>News & Updates </h2> 
<ul>

<div style="height:80px;width:fit-content;overflow:auto;background:#FFFFFF;">
	<li>
		<p>[2023/02/28] One paper on <b>misclassification detection</b> is accepted by <b>CVPR</b> 2023, code is coming soon.
			</p>
	</li>
</div>
</ul>
-->

</p>

<h2>Preprints </h2>
<ul>
<li><p><i>TrustLoRA: Low-Rank Adaptation for Failure Detection under Out-of-distribution Data</i>. ArXiv 2025 [<a href="">paper</a>].<br />
<b>Fei Zhu</b>, Xu-Yao Zhang, Zhaoxiang Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
	
<ul>
<li><p><i>Global Convergence of Continual Learning on Non-IID Data</i>. ArXiv 2025 [<a href="https://arxiv.org/pdf/2503.18511">paper</a>].<br />
<b>Fei Zhu</b>, Yujing Liu, Wenzhuo Liu, Zhaoxiang Zhang. <br /></p>
</li>
</ul>
	
<ul>
<li><p><i>PASS++: A Dual Bias Reduction Framework for Non-Exemplar Class-Incremental Learning</i>. ArXiv 2024 [<a href="https://arxiv.org/pdf/2407.14029">paper</a>].<br />
<b>Fei Zhu</b>, Xu-Yao Zhang, Zhen Cheng, Cheng-Lin Liu. <br /></p>
</li>
</ul>
	
<ul>
<li><p><i>Open-world machine learning: A review and new outlooks</i>. ArXiv 2024 [<a href="https://arxiv.org/pdf/2403.01759">paper</a>].<br />
<b>Fei Zhu</b>, Shijie Ma, Zhen Cheng, Xu-Yao Zhang, Zhaoxiang Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>

<ul>
<li><p><i>Towards Non-Exemplar Semi-Supervised Class-Incremental Learning</i>. ArXiv 2024 [<a href="https://arxiv.org/pdf/2403.18291">paper</a>].<br />
Wenzhuo Liu, <b>Fei Zhu</b>, Cheng-Lin Liu. <br /></p>
</li>
</ul>

<ul>
<li><p><i>Branch-Tuning: Balancing Stability and Plasticity for Continual Self-Supervised Learning</i>. ArXiv 2024 [<a href="https://arxiv.org/pdf/2403.18266">paper</a>].<br />
Wenzhuo Liu, <b>Fei Zhu</b>, Cheng-Lin Liu. <br /></p>
</li>
</ul>

<ul>
<li><p><i>Multi-scale Unified Network for Image Classification</i>. ArXiv 2024 [<a href="https://arxiv.org/pdf/2403.18294">paper</a>].<br />
Wenzhuo Liu, <b>Fei Zhu</b>, Cheng-Lin Liu. <br /></p>
</li>
</ul>

<ul>
<li><p><i>DESIRE: Dynamic Knowledge Consolidation for Rehearsal-Free Continual Learning</i>. ArXiv 2025 [<a href="https://arxiv.org/abs/2411.19154">paper</a>].<br />
Haiyang Guo, <b>Fei Zhu</b>, Fan-hu Zeng, Bing Liu, Xu-Yao Zhang. <br /></p>
</li>
</ul>


<ul>
<li><p><i>Dual-Modality Guided Prompt for Continual Learning of Large Multimodal Models</i>. ArXiv 2025 [<a href="https://arxiv.org/abs/2410.05849">paper</a>].<br />
Fan-hu Zeng, <b>Fei Zhu</b>, Haiyang Guo, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
	


<h2>Selected Publications </h2>
<ul>
<li><p>[TPAMI 2025] <i>ProtoGCD: Unified and Unbiased Prototype Learning for Generalized Category Discovery</i> [<a href="https://ieeexplore.ieee.org/abstract/document/10948388">paper</a>] [<a href="https://github.com/mashijie1028/ProtoGCD">code</a>].<br />
Shijie Ma, <b>Fei Zhu</b>, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>

<ul>
<li><p><i>[ICLR 2025] C-CLIP: Multimodal Continual Learning for Vision-Language Model</i> [<a href="https://openreview.net/forum?id=sb7qHFYwBc">paper</a>].<br />
Wen-Zhuo Liu, <b>Fei Zhu</b><sup>ðŸ“§</sup>, Longhui Wei, Qi Tian. <br /></p>
</li>
</ul>

<ul>
<li><p><i>[TNNLS 2025] Average of Pruning: Improving Performance and Stability of Out-of-Distribution Detection</i> [<a href="https://arxiv.org/abs/2303.01201">paper</a>].<br />
Zhen Cheng, <b>Fei Zhu</b>, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>

<ul>
<li><p>[NeurIPS 2024] <i>MSPE: Multi-Scale Patch Embedding Prompts Vision Transformers to Any Resolution</i> [<a href="https://arxiv.org/pdf/2405.18240">paper</a>].<br />
<b>Wenzhuo Liu<b>, Fei Zhu, Shijie Ma, Cheng-Lin Liu. <br /></p>
</li>
</ul>

<ul>
<li><p>[NeurIPS 2024] <i>Happy: A Debiased Learning Framework for Continual Generalized Category Discovery</i> [<a href="https://proceedings.neurips.cc/paper/2021/file/77ee3bc58ce560b86c2b59363281e914-Paper.pdf">paper</a>] [<a href="https://github.com/mashijie1028/Happy-CGCD">code</a>]. <br />
Shijie Ma, Fei Zhu, <b>Wenzhuo Liu<b>Zhun Zhong, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>

<ul>
<li><p>[ECCV 2024] <i>PILoRA: Prototype Guided Incremental LoRA for Federated Class-Incremental Learning</i> [<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08185.pdf">paper</a>] [<a href="https://arxiv.org/abs/2401.02094">arxiv</a>] [<a href="https://github.com/Ghy0501/PILoRA">code</a>].<br />
Haiyang Guo, Fei Zhu, <b>Wenzhuo Liu<b>, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>



<h2>Academic Services</h2> 
<ul>
<li><p> Conference Reviewer: NeurIPS, ICLR, ICML, CVPR, ICCV, ECCV </p></li>
<li><p> Journal Reviewer: IEEE TIP, TNNLS, TMM, TKDE, PR, NN, IJCV </p></li>
<li><p> Workshop Organizer: Trustworthy Model and Learning in Open Environment, PRCV 2024 </p></li>
</ul>

<h2>Talk</h2> 
<ul>
<li><p> Unknown Rejection in Open Environment, Biomedical Engineering Distinguished Lecture Series, Southern University of Science and Technology, August, 2024 </p></li>
<li><p> Deep Continual Learning, School of Computer Science and Engineering, Nanjing University of Science and Technology, January, 2025  </p></li>
<li><p> Open-Environment Continual Learning, Forum of Zhongguancun College, Beijing, February, 2025  </p></li>
</ul>

</div>
</body>
</html>

<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Wenzhuo Liu</title>

  <meta name="author" content="Wenzhuo Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Wenzhuo Liu</name>
              </p>
              <p>
              I am currently a fifth-year Ph.D. student at the National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences (CASIA), supervised by Prof. <a href="https://nlpr.ia.ac.cn/liucl/" target="_blank">Cheng-Lin Liu</a>.
              <p>
                Before that, I obtained B.Eng. degree from <a href="https://english.bit.edu.cn/">Beijing Institute of Technology</a> in 2020.
              </p>
<!--              Additionally, I have interned at <a href="https://www.meituan.com/">Meituan</a> and <a href="https://www.baai.ac.cn/">BAAI</a>, supervised by Dr. <a href="https://www.xloong.wang/" target="_blank">Xinlong Wang</a>.-->
<!--              </p>-->
            <p>My research aims to improve open-world applications of multimodal foundation models (e.g., CLIP, LLaVA), with particular interests in:</p>

              <li><strong>Open-world learning algorithms</strong>, such as continual pre-training and post-training for MLLMs.</li>
              <li><strong>Open-world MLLM architectures</strong>, including any-resolution and flexible model designs.</li>

              <p style="text-align:center">
                <a href="mailto:liuwenzhuo2020@ia.ac.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=toVhUOgAAAAJ&hl=en"> Google Scholar</a> &nbsp/&nbsp
<!--                <a href="https://github.com/Robertwyq"> Github </a> &nbsp/&nbsp-->
                <a href="data/CV_WenzhuoLiu.pdf">Curriculum Vitae</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:50%;max-width:50%" alt="profile photo" src="img/wenzhuoliu.jpg">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <li style="margin: 5px;" >
                <b>2025-06:</b> <strong>One</strong> paper on visual instruction tuning is accepted to <a href="https://iccv.thecvf.com/">ICCV 2025</a>.
              <li style="margin: 5px;" >
                <b>2025-04:</b> <strong>One</strong> paper on self-supervised learning is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">TNNLS 2025</a>.
              <li style="margin: 5px;" >
                <b>2025-01:</b> <strong>One</strong> paper on vision-language model is accepted to <a href="https://iclr.cc/">ICLR 2025</a>.
              <li style="margin: 5px;" >
                <b>2024-12:</b> <strong>One</strong> paper on prototype learning is accepted to <a href="https://www.sciencedirect.com/journal/pattern-recognition">Pattern Recognition 2025</a>.
              <li style="margin: 5px;" >
                <b>2024-11:</b> <strong>One</strong> paper on pruning for MLLMs is accepted to <a href="https://aaai.org/">AAAI 2025</a>.
              <li style="margin: 5px;" >
                <b>2024-09:</b> <strong>Two</strong> papers on any resolution/generalized category discovery are accepted to <a href="https://neurips.cc/">NeurIPS 2024</a>.
              <li style="margin: 5px;" >
                <b>2024-07:</b> <strong>One</strong> paper on parameter efficient fine-tuning is accepted to <a href="https://eccv.ecva.net/">ECCV 2024</a>.
<!--              <li style="margin: 5px;" >-->
<!--                <b>2024-02:</b> <strong>Two</strong> papers on driving world model and occupancy prediction is accepted to <a href="https://cvpr.thecvf.com/">CVPR 2024</a>.-->
<!--              <li style="margin: 5px;" >-->
<!--                <b>2023-12:</b> <strong>One</strong> paper on multi-agent representation learning is accepted to <a href="https://academic.oup.com/nsr">Nation Science Review</a>.-->
<!--              <li style="margin: 5px;" >-->
<!--                <b>2023-07:</b> <strong>One</strong> paper on unsupervised instance segmentation is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a>.-->
<!--              <li style="margin: 5px;" >-->
<!--                <b>2023-02:</b> <strong>One</strong> paper on 3D object perception is accepted to <a href="http://cvpr2023.thecvf.com/">CVPR 2023</a>.-->
<!--              <li style="margin: 5px;" >-->
<!--                <b>2022-09:</b> <strong>One</strong> paper on unsupervised object discovery is accepted to <a href="https://neurips.cc/">NeurIPS 2022</a>.-->
              </li>
            </p>
          </td>
        </tr>

      </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Preprints</heading>
<!--              <p>-->
<!--                * indicates equal contribution-->
<!--              </p>-->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/llava-c.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>LLaVA-c: Continual Improved Visual Instruction Tuning</papertitle>
                <br>
                <strong>Wenzhuo Liu</strong>, <a>Fei Zhu</a>, <a>Haiyang Guo</a>, <a>Longhui Wei</a>, <a>Cheng-Lin Liu*</a>
                <br>
                <em>arXiv, 2025
                <br>
                <a href="https://arxiv.org/abs/2506.08666">[paper]</a>
                <br>
                <p> An extension of LLaVA-1.5 that incrementally learns new tasks without forgetting, demonstrating that task-by-task learning outperforms multitask pre-training. </p>
            </td>
            </tr>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/msun.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>MSUN: Multi-Scale Unified Network for Native Any Resolutions</papertitle>
                <br>
               <strong>Wenzhuo Liu</strong>, <a>Fei Zhu</a>, <a>Cheng-Lin Liu*</a>
                <br>
                <em>arXiv, 2025
                <br>
                <a href="https://arxiv.org/abs/2403.18294">[paper]</a>
                <br>
                <p> Unified CNN backbone that processes arbitrary input resolutions with robust performance and efficiency.
            </td>
            </tr>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/semi-ipc.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Towards Exemplar-Free Continual Semi-Supervised Learning</papertitle>
                <br>
               <strong>Wenzhuo Liu</strong>, <a>Fei Zhu</a>, <a>Cheng-Lin Liu*</a>
                <br>
                <em>arXiv, 2024
                <br>
                <a href="https://arxiv.org/abs/2403.18291">[paper]</a>
                <br>
                <p> Semi-supervised continual learning framework without data replay. </p>
            </td>
            </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/colt.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Global Convergence of Continual Learning on Non-IID Data</papertitle>
                <br>
               <a>Fei Zhu</a>, <a>Yujing Liu</a>, <strong>Wenzhuo Liu</strong>, <a>Zhaoxiang Zhang*</a>
                <br>
                <em>arXiv, 2025
                <br>
                <a href="https://arxiv.org/abs/2503.18511">[paper]</a>
                <br>
                <p> Provides the general theoretical guarantees for regression continual learning, proving almost-sure convergence and forgetting/regret rate bounds under minimal data assumptions. </p>
            </td>
            </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/RL2.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training</papertitle>
                <br>
                <a>Song Lai</a>, <a>Haohan Zhao</a>, <a>Rong Feng</a>, <a>Changyi Ma</a>, <strong>Wenzhuo Liu</strong>, <a>Hongbo Zhao</a>, <a>Xi Lin</a>, <a>Dong Yi</a>, <a>Min Xie</a>, <a>Qingfu Zhang</a>, <a>Hongbin Liu</a>, <a>Gaofeng Meng</a>, <a>Fei Zhu*</a>
                <br>
                <em>arXiv, 2025
                <br>
                <a href="https://www.arxiv.org/abs/2507.05386">[paper]</a>
                <br>
                <p> A comparative analysis of supervised vs reinforcement fine-tuning, revealing that RFT prevents catastrophic forgetting and preserves general capabilities. </p>
            </td>
            </tr>





        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
<!--              <p>-->
<!--                * indicates equal contribution-->
<!--              </p>-->
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/c-clip.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>C-CLIP: Multimodal Continual Learning for Vision-Language Model</papertitle>
                <br>
                <strong>Wenzhuo Liu</strong>, <a>Fei Zhu*</a>, <a>Qi Tian</a>
                <br>
                <em>ICLR, 2025
                <br>
                <a href="https://openreview.net/pdf?id=sb7qHFYwBc">[paper]</a><a href="https://github.com/SmallPigPeppa/clip-lightning/tree/msun">[Code]</a>
                <br>
                <p> Preserving genernal zero-shot performance of CLIP while adapting to new image-text domains. </p>
            </td>
            </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/mspe.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>MSPE: Multi-Scale Patch Embedding Prompts Vision Transformers to Any Resolution</papertitle>
                <br>
                <strong>Wenzhuo Liu</strong>, <a>Fei Zhu</a>, <a>Shijie Ma</a>, <a>Cheng-Lin Liu*</a>
                <br>
                <em>NIPS, 2024
                <br>
                <a href="https://arxiv.org/abs/2405.18240">[paper]</a><a href="https://github.com/SmallPigPeppa/MSPE-TMP">[Code]</a>
                <br>
                <p> Enable Vision Transformers to natively handle arbitrary input resolutions with minimal modifications. </p>
            </td>
            </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/branch-tuning.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Branch-Tuning: Balancing Stability and Plasticity for Continual Self-Supervised Learning</papertitle>
                <br>
                <strong>Wenzhuo Liu</strong>, <a>Fei Zhu</a>,  <a>Cheng-Lin Liu*</a>
                <br>
                <em>TNNLS, 2025
                <br>
                <a href="https://ieeexplore.ieee.org/document/11073174">[paper]</a><a href="https://github.com/SmallPigPeppa/cassle-latest/tree/tnnls-rebuttal">[Code]</a>
                <br>
                <p> Enable continual self-supervised pretraining without altering existing SSL methods or retaining old data. </p>
            </td>
            </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/ipc.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Class Incremental Learning with Self-Supervised Pre-Training and Prototype Learning</papertitle>
                <br>
                <strong>Wenzhuo Liu</strong>, <a>Xin-Jian Wu</a>, <a>Fei Zhu</a>, <a>Ming-Ming Yu</a>, <a>Chuang Wang</a>, <a>Cheng-Lin Liu*</a>
                <br>
                <em>Pattern Recognition, 2025
                <br>
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320324006940">[paper]</a><a href="https://github.com/SmallPigPeppa/Incremental-CPN/tree/clean">[Code]</a>
                <br>
                <p> Using a self-supervised encoder and incremental prototype classifier to prevent forgetting. </p>
            </td>
            </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/iccv.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Federated Continual Instruction Tuning</papertitle>
                <br>
                <a>Haiyang Guo</a>, <a>Fanhu Zeng</a>, <a>Fei Zhu</a>, <strong>Wenzhuo Liu</strong>, <a>Jian Xu</a>, <a>Xu-Yao Zhang</a>, <a>Cheng-Lin Liu*</a>
                <br>
                <em>ICCV, 2025
                <br>
                <a href="https://arxiv.org/abs/2503.12897">[paper]</a><a href="https://github.com/Ghy0501/FCIT">[Code]</a>
                <br>
                <p> Continual instruction tuning with dynamic knowledge organization and subspace selective activation. </p>
            </td>
            </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/aaai.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information</papertitle>
                <br>
                <a>Yi Chen</a>, <a>Jian Xu</a>, <a>Xu-Yao Zhang</a>, <strong>Wen-Zhuo Liu</strong>, <a>Yang-Yang Liu</a>, <a>Cheng-Lin Liu*</a>
                <br>
                <em>AAAI, 2025
                <br>
                <a href="https://arxiv.org/abs/2409.01179">[paper]</a><a href="https://github.com/banjiuyufen/RecoverableCompression">[Code]</a>
                <br>
                <p> Text-guided dynamic visual token pruning with negligible performance drop in MLLMs. </p>
            </td>
            </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/happy.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Happy: A Debiased Learning Framework for Continual Generalized Category Discovery</papertitle>
                <br>
                <a>Shijie Ma</a>, <a>Fei Zhu</a>, <strong>Wenzhuo Liu</strong>, <a>Zhun Zhong</a>, <a>Xu-Yao Zhang</a>, <a>Cheng-Lin Liu*</a>
                <br>
                <em>NeurIPS, 2024
                <br>
                <a href="https://arxiv.org/abs/2410.06535">[paper]</a><a href="https://github.com/mashijie1028/Happy-CGCD">[Code]</a>
                <br>
                <p> A continual generalized category discovery framework that uncovers new classes without forgetting. </p>
            </td>
            </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/pilora.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Pilora: Prototype-Guided Incremental LoRA for Federated Class-Incremental Learning</papertitle>
                <br>
                <a>Haiyang Guo</a>, <a>Fei Zhu</a>, <strong>Wenzhuo Liu</strong>, <a>Xu-Yao Zhang</a>, <a>Cheng-Lin Liu*</a>
                <br>
                <em>ECCV, 2024
                <br>
                <a href="https://arxiv.org/abs/2401.02094">[paper]</a><a href="https://github.com/Ghy0501/PILoRA">[Code]</a>
                <br>
                <p> Incrementally update LoRA via prototype re-weighting. </p>
            </td>
            </tr>




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Academic Services</heading>
                <p>
                  <li style="margin: 5px;">Conference Reviewer: NeurIPS, ICLR, ICCV, ECCV</li>
                  <li style="margin: 5px;">Journal Reviewer: IEEE TIP, TNNLS, TMM, TMC, PR, NN</li>
                  <li style="margin: 5px;">Workshop Organizer: Trustworthy Model and Learning in Open Environment, PRCV 2024</li>
                </p>
              </td>
            </tr>
          </tbody>
        </table>

<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--            <tr>-->
<!--            <td style="padding:20px;width:100%;vertical-align:middle">-->
<!--              <heading>Honors and Awards</heading>-->
<!--              <p>-->
<!--                <li style="margin: 5px;"> 2025 中科院院长奖 </li>-->
<!--                <li style="margin: 5px;"> 2025 北京市优秀毕业生 </li>-->
<!--                <li style="margin: 5px;"> 2024 National Scholarship / 国家奖学金 </li>-->
<!--                <li style="margin: 5px;"> 2023 朱李月华奖学金 </li>-->
<!--                <li style="margin: 5px;"> 2020 浙江省优秀毕业生 </li>-->
<!--                <li style="margin: 5px;"> 2018 National Scholarship / 国家奖学金 </li>-->
<!--                <li style="margin: 5px;"> 2018 中控奖学金 </li>-->
<!--              </p>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

<p><center>
	  <div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=pYUe-9vNee5nJl9ztd0lgo-xYiNaKRYwhjvT3xnX5Mg"></script>
	  </div>
	  <br>
	    &copy; Wenzhuo Liu | Last updated: June 26, 2025
</center></p>
</body>

</html>